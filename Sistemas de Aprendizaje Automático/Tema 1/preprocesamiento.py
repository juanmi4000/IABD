# -*- coding: utf-8 -*-
"""Tarea1_SAA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Pvn8tdXzozrRjyAe3t9dgr84wguHghVC
"""

import pandas as pd
import scipy.stats as st
import numpy as np
import math as mt
from sklearn.preprocessing import MinMaxScaler

# Ejercicio 1
tamanio = 1000 # Varaible para el tamaño de los dataframe

x = np.random.normal(35, 5, tamanio) # Variable x, 1000 datos, media 35 y desviación típica 5
y = np.random.normal(100, 15, tamanio) # Variable y, 1000 datos, media 100 y desviación típica 15
z = np.random.normal(25000, 5000, tamanio)# Variable x, 1000 datos, media 25000 y desviación típica 5000

dataframe = pd.DataFrame({"VariableX": x, "VariableY" : y, "VariableZ" : z}) # Creo el dataframe usando las variables anteriores y nombre nombre a las columnas
print(dataframe)

# Ejercicio 2
print(dataframe.describe()) # describe() muestra los datos estadísticos para cada columna

# Ejercicio 3
mediaY = dataframe["VariableY"].mean() # Media
sigmaY = dataframe["VariableY"].std() # Desviación típica
a = (55 - mediaY) / sigmaY
b = (70  - mediaY) / sigmaY
probY = st.norm.cdf(b) - st.norm.cdf(a) # Probabilidad de que se encuentre entre 55 y 70
print(f"La probabilidad de que esté entre 55 y 70 es {probY}")

mediaZ = dataframe.VariableZ.mean() # Media
sigmaZ = dataframe.VariableZ.std() # Desviación típica
c = (20000 - mediaZ) / sigmaZ
d = (30000  - mediaZ) / sigmaZ
probZ = st.norm.cdf(b) - st.norm.cdf(a) # Propabilidad entre 0 y 1. Probabilidad de se de encuentre entre 20000 y 30000
print(f"La probabilidad de que esté entre 20000 y 30000 es {probZ}")

# Ejercicio 4
def detectarLimitesOutlier(df, nombreColumna, k = 3):
  Q1 = np.quantile(df[nombreColumna].astype(float), 0.25) # Calculo el quantil 1
  Q3 = np.quantile(df[nombreColumna].astype(float), 0.75) # Calculo el quantil 3
  IQR = Q3 - Q1
  xL = Q1 - k * IQR # Límite inferior
  xU = Q3 + k * IQR # Límite Superior
  return (xL, xU)

limiteInferior, limiteSuperior = detectarLimitesOutlier(dataframe, "VariableX") # Utilizo la desestructuración
print(f"LIMITES INFERIORES Y SUPERIORES:\nLímite inferior: {limiteInferior}\nLímite superior: {limiteSuperior}")

# Ejercicio 5
# dataframe['VariableX'][50] = 200 # Esto en futuras versiones va a cambiar y de debe hacer con df.loc[row_indexer, "col"]
# Otra forma de acceder a la fila 50 y la columna VariableX
# Introduzco dos valores deliberadamente por cada variable
dataframe.at[50, "VariableX"] = 200
dataframe.at[800, "VariableX"] = -230
dataframe.at[34, "VariableY"] = -40
dataframe.at[654, "VariableY"] = 525
dataframe.at[978, "VariableZ"] = 1000234
dataframe.at[12, "VariableZ"] = -3450
print(dataframe.at[12, "VariableZ"])
#print(f"{dataframe.at[50, "VariableX"]}, {dataframe.at[800, "VariableX"]} {dataframe.at[34, "VariableY"]}, {dataframe.at[654, "VariableY"] = 525}, {dataframe.at[978, "VariableZ"] = 1000234}, {dataframe.at[12, "VariableZ"]}")

longitud = len(dataframe["VariableX"])
mediasX = np.zeros(longitud)
mediasY = np.zeros(longitud)
mediasZ = np.zeros(longitud)
for i in range(longitud):
  datos_x_sin_i = np.delete(dataframe.VariableX, i)
  datos_y_sin_i = np.delete(dataframe.VariableY, i)
  datos_z_sin_i = np.delete(dataframe.VariableZ, i)
  mediasX[i] = np.mean(datos_x_sin_i)
  mediasY[i] = np.mean(datos_y_sin_i)
  mediasZ[i] = np.mean(datos_z_sin_i)

limiteInferiorX, limiteSuperiorX = detectarLimitesOutlier(dataframe, "VariableX")
limiteInferiorY, limiteSuperiorY = detectarLimitesOutlier(dataframe, "VariableY")
limiteInferiorZ, limiteSuperiorZ = detectarLimitesOutlier(dataframe, "VariableZ")

# Ejercicio 6
print(dataframe)
scaler = MinMaxScaler()
print(scaler.fit(dataframe))
MinMaxScaler()
print(scaler.data_max_)
print(scaler.transform(dataframe))
#print(scaler.transform([[2, 2]]))

def detectarLimitesOutlier2(df, k = 3):
  Q1 = np.quantile(df, 0.25) # Calculo el quantil 1
  Q3 = np.quantile(df, 0.75) # Calculo el quantil 3
  IQR = Q3 - Q1
  xL = Q1 - k * IQR # Límite inferior
  xU = Q3 + k * IQR # Límite Superior
  return (xL, xU)

# Ejercicio 7
# El dataframe ha sido sacado del siguiente enlace: https://www.kaggle.com/code/chaitanya99/recommendation-system-cf-anime/
# Apartado 1:
anime_df = pd.read_csv("./anime.csv") # Import el dataframe (lo he subido a colab)

# Apartado 2:
episodios = anime_df.Episodes.apply(pd.to_numeric, errors='coerce').dropna() # Obtengo la columna de episodios
score = anime_df['Score'].apply(pd.to_numeric, errors='coerce').dropna() # Obtengo la columna del score

# Apartado 3:
# MEDIA Y DESVIACIÓN TÍPICA DE EPISODIOS
mediaEpisodios = episodios.mean() # Media de la variable episodios ignorando los datos que no sean float
desTipEpisodios = episodios.std() # Desviación típica de la variable episodios ignorando los datos que no sean float

# MEDIA Y DESVIACIÓN TÍPICA DE SCORE
mediaScore = score.mean() # Media de la variable score ignorando los datos que no sean float
desTipScore = score.std() # Desviación típica de la variable score ignorando los datos que no sean float

# Apartado 4 y 5:
# Sin modificar los datos de episodios y score debería de haber outliers porque la desviación típica se aleja bastante de la media, pero no entiendo porque no me salen ni aunque ponga datos que se alejan.
episodios.loc[3] = 100000
score.loc[3] = 4000
mediasEpisodios = np.zeros(len(episodios))
mediasScore = np.zeros(len(score))
for i in range(len(episodios)):
  datos_episodios_sin_i = np.delete(episodios, i)
  mediasEpisodios[i] = np.mean(datos_episodios_sin_i)

for j in range(len(score)):
  datos_score_sin_j = np.delete(score, j)
  mediasScore[j] = np.mean(datos_score_sin_j)

xLE, xUE = detectarLimitesOutlier2(episodios, 1.5) # Obtengo los límites de los episodios
xLS, xUS = detectarLimitesOutlier2(score, 1.5) # Obtengo los límites del score

for x in range(len(episodios)):
    if mediasEpisodios[x] < xLE or mediasEpisodios[x] > xUE:
        print(f"La media[{x}] de episodios = {mediasEpisodios[x]} es influyente.")

for y in range(len(score)):
    if mediasScore[y] < xLS or mediasScore[y] > xUS:
        print(f"La media[{y}] de score = {mediasScore[y]} es influyente.")

# Apartado 6:
# Reemplazar 'Unknown' por NaN
anime_df['Episodes'] = anime_df['Episodes'].replace('Unknown', np.nan)
anime_df['Score'] = anime_df['Score'].replace('Unknown', np.nan)

# Eliminar filas con NaN
anime_df = anime_df.dropna(subset=['Episodes'])
anime_df = anime_df.dropna(subset=['Score'])

# Escalar la columna 'Episodes' y 'Score'
scaler = MinMaxScaler()
episodiosEscalados = scaler.fit_transform(anime_df['Episodes'].astype(float).values.reshape(-1, 1))
scoreEscalado = scaler.fit_transform(anime_df['Episodes'].astype(float).values.reshape(-1, 1))

# Mostrar los resultados
print(episodiosEscalados)
print(scoreEscalado)